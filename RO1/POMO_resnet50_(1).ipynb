{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "POMO_resnet50 (1).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FeaZiPSvJdJv",
        "outputId": "6fa88a40-8048-42f1-f043-a2ab0fa306c7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9BMC09UcroJ9",
        "outputId": "c1406553-f767-4dca-b8a9-d14f33d70c67"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jul 23 09:49:11 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FmSyT7xsN5v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2248f1ea-5076-41f2-bafd-b76f1df61b97"
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-c823bcd5-0af9-0366-bcbb-a54d28b58c48)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqJXv0fnx0Du",
        "outputId": "0a2e486d-0a78-438d-9f5d-1ea09fbfe482"
      },
      "source": [
        "# To unzip \n",
        "from zipfile import ZipFile\n",
        "file_name = \"/content/drive/MyDrive/Colab Notebooks/data_64.zip\"\n",
        "with ZipFile(file_name, 'r') as zip:\n",
        "  zip.extractall()\n",
        "  print('done')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlayLmeDx0Gm"
      },
      "source": [
        "# import the libraries as shown below\n",
        " \n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten,Conv2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.models import load_model\n",
        "from keras.preprocessing import image\n",
        "import os, time, shutil\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Z97JWKXx0Jq"
      },
      "source": [
        "# re-size all the images to this\n",
        "IMAGE_SIZE = [224, 224]\n",
        " \n",
        "train_path = '/content/data_64/train'\n",
        "valid_path = '/content/data_64/test'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iGslSlDx0MK",
        "outputId": "3bcce477-afde-4158-96ab-bb4f8a526783"
      },
      "source": [
        "# Import the Vgg 16 library as shown below and add preprocessing layer to the front of VGG\n",
        "# Here we will be using imagenet weights\n",
        " \n",
        "ResNet50_model = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "\n",
        "# don't train existing weights\n",
        "for layer in ResNet50_model.layers:\n",
        "    layer.trainable = False\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a-QCSFYHx0O_",
        "outputId": "e8790f69-35c5-4fea-c535-b1ef7ed67b8d"
      },
      "source": [
        "# useful for getting number of output classes\n",
        "folders = glob('/content/data_64/train/*')\n",
        "folders"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/data_64/train/5',\n",
              " '/content/data_64/train/3',\n",
              " '/content/data_64/train/0',\n",
              " '/content/data_64/train/1',\n",
              " '/content/data_64/train/2',\n",
              " '/content/data_64/train/4']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mwbJYb_x0Rm"
      },
      "source": [
        "# our layers - you can add more if you want\n",
        "x = Flatten()(ResNet50_model.output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Pg5mUIxx0Uv"
      },
      "source": [
        "prediction = Dense(len(folders), activation='softmax')(x)\n",
        " \n",
        "# create a model object\n",
        "model = Model(inputs=ResNet50_model.input, outputs=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giH6FTRAx0W5",
        "outputId": "8b37e734-fe54-4b92-9898-cb735b9e37b9"
      },
      "source": [
        "# view the structure of the model\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100352)       0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 6)            602118      flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 24,189,830\n",
            "Trainable params: 602,118\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FETEFoAyx0Z0"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        " \n",
        "# opt = keras.optimizers.Adam(learning_rate=0.01)\n",
        "# model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCvh6DF8x0c0"
      },
      "source": [
        "# Checkpoint's \n",
        "# https://keras.io/api/callbacks/model_checkpoint/\n",
        " \n",
        "filepath=\"model_ResNet50-{epoch:02d}-{val_accuracy:.3f}-{val_loss:.3f}.h5\"\n",
        "checkpoint = ModelCheckpoint(filepath, \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode='min')\n",
        "callbacks_list = [checkpoint]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GNjwFzbx0fT"
      },
      "source": [
        "# Use the Image Data Generator to import the images from the dataset\n",
        "# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        " \n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.1,\n",
        "                                   zoom_range = 0.1,\n",
        "                                   horizontal_flip = True)\n",
        " \n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAWn_yYfx0hj",
        "outputId": "9e9a3f69-ffde-4a1a-b685-5d586060fa5b"
      },
      "source": [
        "# Make sure you provide the same target size as initialied for the image size\n",
        "training_set = train_datagen.flow_from_directory('/content/data_64/train',\n",
        "                                                 target_size = (224, 224),\n",
        "                                                 batch_size = 128,\n",
        "                                                 class_mode = 'categorical')\n",
        " \n",
        "test_set = test_datagen.flow_from_directory('/content/data_64/test',\n",
        "                                            target_size = (224, 224),\n",
        "                                            batch_size = 128,\n",
        "                                            class_mode = 'categorical')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 5787 images belonging to 6 classes.\n",
            "Found 3859 images belonging to 6 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7dp4UJ2x0kW",
        "outputId": "08c25c2f-1fa1-47e9-a20e-848a183cd518"
      },
      "source": [
        "# fit the model\n",
        "# Run the cell. It will take some time to execute\n",
        " \n",
        "start = time.time()\n",
        " \n",
        "r = model.fit_generator(\n",
        "  training_set,\n",
        "  validation_data=test_set,\n",
        "  epochs=150,\n",
        "  steps_per_epoch=len(training_set),\n",
        "  validation_steps=len(test_set),\n",
        "  callbacks=callbacks_list\n",
        ")\n",
        "print('\\n\\n')\n",
        "print(f'Time: {time.time() - start}')\n",
        " \n",
        "model.save('ResNet50_model.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py:1940: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "46/46 [==============================] - 141s 2s/step - loss: 5.9828 - accuracy: 0.2535 - val_loss: 1.9383 - val_accuracy: 0.2545\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.93833, saving model to model_ResNet50-01-0.254-1.938.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
            "  category=CustomMaskWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 2/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 1.4348 - accuracy: 0.4458 - val_loss: 1.3362 - val_accuracy: 0.4499\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.93833 to 1.33615, saving model to model_ResNet50-02-0.450-1.336.h5\n",
            "Epoch 3/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 1.3260 - accuracy: 0.4882 - val_loss: 1.2222 - val_accuracy: 0.5431\n",
            "\n",
            "Epoch 00003: val_loss improved from 1.33615 to 1.22219, saving model to model_ResNet50-03-0.543-1.222.h5\n",
            "Epoch 4/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 1.2577 - accuracy: 0.5127 - val_loss: 1.3073 - val_accuracy: 0.5382\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 1.22219\n",
            "Epoch 5/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 1.2076 - accuracy: 0.5403 - val_loss: 1.0916 - val_accuracy: 0.5981\n",
            "\n",
            "Epoch 00005: val_loss improved from 1.22219 to 1.09157, saving model to model_ResNet50-05-0.598-1.092.h5\n",
            "Epoch 6/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 1.1281 - accuracy: 0.5600 - val_loss: 1.0696 - val_accuracy: 0.6095\n",
            "\n",
            "Epoch 00006: val_loss improved from 1.09157 to 1.06956, saving model to model_ResNet50-06-0.609-1.070.h5\n",
            "Epoch 7/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 1.0973 - accuracy: 0.5856 - val_loss: 1.3032 - val_accuracy: 0.5481\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 1.06956\n",
            "Epoch 8/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 1.1549 - accuracy: 0.5587 - val_loss: 1.2824 - val_accuracy: 0.4408\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 1.06956\n",
            "Epoch 9/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 1.1298 - accuracy: 0.5772 - val_loss: 1.2180 - val_accuracy: 0.4864\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 1.06956\n",
            "Epoch 10/150\n",
            "46/46 [==============================] - 105s 2s/step - loss: 1.0399 - accuracy: 0.6069 - val_loss: 1.0109 - val_accuracy: 0.5981\n",
            "\n",
            "Epoch 00010: val_loss improved from 1.06956 to 1.01091, saving model to model_ResNet50-10-0.598-1.011.h5\n",
            "Epoch 11/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.9896 - accuracy: 0.6209 - val_loss: 0.9208 - val_accuracy: 0.6828\n",
            "\n",
            "Epoch 00011: val_loss improved from 1.01091 to 0.92079, saving model to model_ResNet50-11-0.683-0.921.h5\n",
            "Epoch 12/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 0.9586 - accuracy: 0.6328 - val_loss: 0.9379 - val_accuracy: 0.6613\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.92079\n",
            "Epoch 13/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.9665 - accuracy: 0.6477 - val_loss: 0.9595 - val_accuracy: 0.6675\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.92079\n",
            "Epoch 14/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.9228 - accuracy: 0.6641 - val_loss: 0.9152 - val_accuracy: 0.6535\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.92079 to 0.91524, saving model to model_ResNet50-14-0.654-0.915.h5\n",
            "Epoch 15/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.9431 - accuracy: 0.6423 - val_loss: 0.8873 - val_accuracy: 0.6554\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.91524 to 0.88734, saving model to model_ResNet50-15-0.655-0.887.h5\n",
            "Epoch 16/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.9017 - accuracy: 0.6720 - val_loss: 0.8632 - val_accuracy: 0.6968\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.88734 to 0.86319, saving model to model_ResNet50-16-0.697-0.863.h5\n",
            "Epoch 17/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.9698 - accuracy: 0.6248 - val_loss: 1.0054 - val_accuracy: 0.6789\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.86319\n",
            "Epoch 18/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.9818 - accuracy: 0.6440 - val_loss: 1.1632 - val_accuracy: 0.5237\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.86319\n",
            "Epoch 19/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.8994 - accuracy: 0.6646 - val_loss: 0.7886 - val_accuracy: 0.7346\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.86319 to 0.78856, saving model to model_ResNet50-19-0.735-0.789.h5\n",
            "Epoch 20/150\n",
            "46/46 [==============================] - 111s 2s/step - loss: 0.7525 - accuracy: 0.7287 - val_loss: 0.9756 - val_accuracy: 0.6450\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.78856\n",
            "Epoch 21/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 0.8795 - accuracy: 0.6743 - val_loss: 1.0514 - val_accuracy: 0.6209\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.78856\n",
            "Epoch 22/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.9034 - accuracy: 0.6634 - val_loss: 0.8896 - val_accuracy: 0.7103\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.78856\n",
            "Epoch 23/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.8921 - accuracy: 0.6670 - val_loss: 0.8504 - val_accuracy: 0.7118\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.78856\n",
            "Epoch 24/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.7967 - accuracy: 0.7005 - val_loss: 0.8422 - val_accuracy: 0.6909\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.78856\n",
            "Epoch 25/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.7901 - accuracy: 0.7054 - val_loss: 0.7888 - val_accuracy: 0.7147\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.78856\n",
            "Epoch 26/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.7441 - accuracy: 0.7335 - val_loss: 1.0698 - val_accuracy: 0.6611\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.78856\n",
            "Epoch 27/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 0.8139 - accuracy: 0.7019 - val_loss: 0.8935 - val_accuracy: 0.6655\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.78856\n",
            "Epoch 28/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 0.7048 - accuracy: 0.7420 - val_loss: 0.8388 - val_accuracy: 0.6919\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.78856\n",
            "Epoch 29/150\n",
            "46/46 [==============================] - 110s 2s/step - loss: 0.7816 - accuracy: 0.7142 - val_loss: 0.7207 - val_accuracy: 0.7554\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.78856 to 0.72074, saving model to model_ResNet50-29-0.755-0.721.h5\n",
            "Epoch 30/150\n",
            "46/46 [==============================] - 110s 2s/step - loss: 0.7447 - accuracy: 0.7258 - val_loss: 0.7820 - val_accuracy: 0.7217\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.72074\n",
            "Epoch 31/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.7733 - accuracy: 0.7099 - val_loss: 1.2023 - val_accuracy: 0.5509\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.72074\n",
            "Epoch 32/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.8559 - accuracy: 0.6860 - val_loss: 0.6615 - val_accuracy: 0.7841\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.72074 to 0.66154, saving model to model_ResNet50-32-0.784-0.662.h5\n",
            "Epoch 33/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.7011 - accuracy: 0.7437 - val_loss: 0.8689 - val_accuracy: 0.6903\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.66154\n",
            "Epoch 34/150\n",
            "46/46 [==============================] - 110s 2s/step - loss: 0.7505 - accuracy: 0.7247 - val_loss: 0.7164 - val_accuracy: 0.7590\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.66154\n",
            "Epoch 35/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.7001 - accuracy: 0.7422 - val_loss: 0.7590 - val_accuracy: 0.7388\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.66154\n",
            "Epoch 36/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.6827 - accuracy: 0.7546 - val_loss: 0.7872 - val_accuracy: 0.7455\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.66154\n",
            "Epoch 37/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.6665 - accuracy: 0.7621 - val_loss: 0.7740 - val_accuracy: 0.7274\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.66154\n",
            "Epoch 38/150\n",
            "46/46 [==============================] - 110s 2s/step - loss: 0.6621 - accuracy: 0.7560 - val_loss: 0.7098 - val_accuracy: 0.7707\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66154\n",
            "Epoch 39/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.6714 - accuracy: 0.7565 - val_loss: 0.7496 - val_accuracy: 0.7253\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.66154\n",
            "Epoch 40/150\n",
            "46/46 [==============================] - 110s 2s/step - loss: 0.6749 - accuracy: 0.7510 - val_loss: 0.7716 - val_accuracy: 0.7245\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.66154\n",
            "Epoch 41/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.7179 - accuracy: 0.7297 - val_loss: 0.6448 - val_accuracy: 0.7795\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.66154 to 0.64478, saving model to model_ResNet50-41-0.779-0.645.h5\n",
            "Epoch 42/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.5979 - accuracy: 0.7871 - val_loss: 0.6591 - val_accuracy: 0.7901\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.64478\n",
            "Epoch 43/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 0.6881 - accuracy: 0.7460 - val_loss: 0.8284 - val_accuracy: 0.7243\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.64478\n",
            "Epoch 44/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.6805 - accuracy: 0.7484 - val_loss: 0.8308 - val_accuracy: 0.7256\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.64478\n",
            "Epoch 45/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.6414 - accuracy: 0.7627 - val_loss: 0.7505 - val_accuracy: 0.7406\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.64478\n",
            "Epoch 46/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.6078 - accuracy: 0.7850 - val_loss: 0.6134 - val_accuracy: 0.8072\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.64478 to 0.61342, saving model to model_ResNet50-46-0.807-0.613.h5\n",
            "Epoch 47/150\n",
            "46/46 [==============================] - 109s 2s/step - loss: 0.6399 - accuracy: 0.7648 - val_loss: 0.6256 - val_accuracy: 0.7818\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.61342\n",
            "Epoch 48/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.6682 - accuracy: 0.7427 - val_loss: 0.6171 - val_accuracy: 0.8010\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.61342\n",
            "Epoch 49/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 0.5931 - accuracy: 0.7926 - val_loss: 0.7101 - val_accuracy: 0.7362\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.61342\n",
            "Epoch 50/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 0.5595 - accuracy: 0.7975 - val_loss: 0.5923 - val_accuracy: 0.8228\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.61342 to 0.59233, saving model to model_ResNet50-50-0.823-0.592.h5\n",
            "Epoch 51/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.5708 - accuracy: 0.7921 - val_loss: 0.6230 - val_accuracy: 0.8031\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.59233\n",
            "Epoch 52/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.6072 - accuracy: 0.7826 - val_loss: 0.6447 - val_accuracy: 0.7803\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.59233\n",
            "Epoch 53/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.5747 - accuracy: 0.7907 - val_loss: 0.5853 - val_accuracy: 0.8176\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.59233 to 0.58535, saving model to model_ResNet50-53-0.818-0.585.h5\n",
            "Epoch 54/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.6418 - accuracy: 0.7674 - val_loss: 0.7363 - val_accuracy: 0.7624\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.58535\n",
            "Epoch 55/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.5892 - accuracy: 0.7888 - val_loss: 0.7977 - val_accuracy: 0.7023\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.58535\n",
            "Epoch 56/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.8308 - accuracy: 0.7028 - val_loss: 0.7255 - val_accuracy: 0.7349\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.58535\n",
            "Epoch 57/150\n",
            "46/46 [==============================] - 105s 2s/step - loss: 0.6469 - accuracy: 0.7653 - val_loss: 0.7471 - val_accuracy: 0.7328\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.58535\n",
            "Epoch 58/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.5940 - accuracy: 0.7864 - val_loss: 0.7170 - val_accuracy: 0.7492\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.58535\n",
            "Epoch 59/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.5647 - accuracy: 0.7930 - val_loss: 0.7083 - val_accuracy: 0.7810\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.58535\n",
            "Epoch 60/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.5969 - accuracy: 0.7843 - val_loss: 0.5875 - val_accuracy: 0.8126\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.58535\n",
            "Epoch 61/150\n",
            "46/46 [==============================] - 107s 2s/step - loss: 0.6540 - accuracy: 0.7691 - val_loss: 0.7242 - val_accuracy: 0.7453\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.58535\n",
            "Epoch 62/150\n",
            "46/46 [==============================] - 108s 2s/step - loss: 0.5464 - accuracy: 0.8008 - val_loss: 0.5966 - val_accuracy: 0.7958\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.58535\n",
            "Epoch 63/150\n",
            "46/46 [==============================] - 105s 2s/step - loss: 0.5512 - accuracy: 0.7968 - val_loss: 0.7474 - val_accuracy: 0.7517\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.58535\n",
            "Epoch 64/150\n",
            "46/46 [==============================] - 106s 2s/step - loss: 0.5923 - accuracy: 0.7774 - val_loss: 0.6146 - val_accuracy: 0.7981\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.58535\n",
            "Epoch 65/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.6219 - accuracy: 0.7821 - val_loss: 0.6221 - val_accuracy: 0.7930\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.58535\n",
            "Epoch 66/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.5567 - accuracy: 0.7975 - val_loss: 0.5845 - val_accuracy: 0.8225\n",
            "\n",
            "Epoch 00066: val_loss improved from 0.58535 to 0.58454, saving model to model_ResNet50-66-0.822-0.585.h5\n",
            "Epoch 67/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5299 - accuracy: 0.8072 - val_loss: 0.5628 - val_accuracy: 0.8347\n",
            "\n",
            "Epoch 00067: val_loss improved from 0.58454 to 0.56278, saving model to model_ResNet50-67-0.835-0.563.h5\n",
            "Epoch 68/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.5691 - accuracy: 0.7932 - val_loss: 0.5872 - val_accuracy: 0.7989\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.56278\n",
            "Epoch 69/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5120 - accuracy: 0.8158 - val_loss: 0.6245 - val_accuracy: 0.7873\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.56278\n",
            "Epoch 70/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.5665 - accuracy: 0.7909 - val_loss: 0.7927 - val_accuracy: 0.6859\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.56278\n",
            "Epoch 71/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5586 - accuracy: 0.7971 - val_loss: 0.5805 - val_accuracy: 0.8181\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.56278\n",
            "Epoch 72/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4903 - accuracy: 0.8239 - val_loss: 0.6515 - val_accuracy: 0.7683\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.56278\n",
            "Epoch 73/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5282 - accuracy: 0.8053 - val_loss: 0.8416 - val_accuracy: 0.6989\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.56278\n",
            "Epoch 74/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5127 - accuracy: 0.8189 - val_loss: 0.5767 - val_accuracy: 0.8056\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.56278\n",
            "Epoch 75/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.5451 - accuracy: 0.7961 - val_loss: 0.5706 - val_accuracy: 0.8215\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.56278\n",
            "Epoch 76/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5486 - accuracy: 0.7970 - val_loss: 0.5311 - val_accuracy: 0.8290\n",
            "\n",
            "Epoch 00076: val_loss improved from 0.56278 to 0.53106, saving model to model_ResNet50-76-0.829-0.531.h5\n",
            "Epoch 77/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.5238 - accuracy: 0.8058 - val_loss: 0.5187 - val_accuracy: 0.8424\n",
            "\n",
            "Epoch 00077: val_loss improved from 0.53106 to 0.51868, saving model to model_ResNet50-77-0.842-0.519.h5\n",
            "Epoch 78/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.6013 - accuracy: 0.7809 - val_loss: 0.6453 - val_accuracy: 0.7891\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.51868\n",
            "Epoch 79/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5187 - accuracy: 0.8013 - val_loss: 0.6978 - val_accuracy: 0.7896\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.51868\n",
            "Epoch 80/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.5592 - accuracy: 0.7907 - val_loss: 0.6976 - val_accuracy: 0.7551\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.51868\n",
            "Epoch 81/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.5478 - accuracy: 0.7951 - val_loss: 0.6388 - val_accuracy: 0.8023\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.51868\n",
            "Epoch 82/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5046 - accuracy: 0.8129 - val_loss: 0.5583 - val_accuracy: 0.8181\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.51868\n",
            "Epoch 83/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.4636 - accuracy: 0.8377 - val_loss: 0.5772 - val_accuracy: 0.8129\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.51868\n",
            "Epoch 84/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4977 - accuracy: 0.8218 - val_loss: 0.5571 - val_accuracy: 0.8292\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.51868\n",
            "Epoch 85/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4660 - accuracy: 0.8372 - val_loss: 0.5254 - val_accuracy: 0.8217\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.51868\n",
            "Epoch 86/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.4378 - accuracy: 0.8497 - val_loss: 0.5017 - val_accuracy: 0.8593\n",
            "\n",
            "Epoch 00086: val_loss improved from 0.51868 to 0.50169, saving model to model_ResNet50-86-0.859-0.502.h5\n",
            "Epoch 87/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4617 - accuracy: 0.8351 - val_loss: 0.5506 - val_accuracy: 0.8217\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.50169\n",
            "Epoch 88/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4477 - accuracy: 0.8429 - val_loss: 0.7062 - val_accuracy: 0.7668\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.50169\n",
            "Epoch 89/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5701 - accuracy: 0.7833 - val_loss: 0.6349 - val_accuracy: 0.7779\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.50169\n",
            "Epoch 90/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4500 - accuracy: 0.8424 - val_loss: 0.5718 - val_accuracy: 0.8269\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.50169\n",
            "Epoch 91/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.5720 - accuracy: 0.7918 - val_loss: 0.8164 - val_accuracy: 0.7847\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.50169\n",
            "Epoch 92/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5057 - accuracy: 0.8215 - val_loss: 0.6290 - val_accuracy: 0.7836\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.50169\n",
            "Epoch 93/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.4879 - accuracy: 0.8248 - val_loss: 0.5777 - val_accuracy: 0.8145\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.50169\n",
            "Epoch 94/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5308 - accuracy: 0.8008 - val_loss: 0.5596 - val_accuracy: 0.8196\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.50169\n",
            "Epoch 95/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4680 - accuracy: 0.8312 - val_loss: 0.6548 - val_accuracy: 0.7961\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.50169\n",
            "Epoch 96/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.4710 - accuracy: 0.8301 - val_loss: 0.5943 - val_accuracy: 0.8090\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.50169\n",
            "Epoch 97/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.5434 - accuracy: 0.8018 - val_loss: 0.5708 - val_accuracy: 0.8126\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.50169\n",
            "Epoch 98/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4376 - accuracy: 0.8452 - val_loss: 0.5646 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.50169\n",
            "Epoch 99/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4343 - accuracy: 0.8459 - val_loss: 0.5605 - val_accuracy: 0.8209\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.50169\n",
            "Epoch 100/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4338 - accuracy: 0.8445 - val_loss: 0.4855 - val_accuracy: 0.8632\n",
            "\n",
            "Epoch 00100: val_loss improved from 0.50169 to 0.48551, saving model to model_ResNet50-100-0.863-0.486.h5\n",
            "Epoch 101/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.4067 - accuracy: 0.8597 - val_loss: 0.5922 - val_accuracy: 0.8046\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.48551\n",
            "Epoch 102/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4268 - accuracy: 0.8493 - val_loss: 0.6962 - val_accuracy: 0.7813\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.48551\n",
            "Epoch 103/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4821 - accuracy: 0.8213 - val_loss: 0.7349 - val_accuracy: 0.6986\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.48551\n",
            "Epoch 104/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.5234 - accuracy: 0.8110 - val_loss: 0.6432 - val_accuracy: 0.7976\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.48551\n",
            "Epoch 105/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5395 - accuracy: 0.7925 - val_loss: 0.5706 - val_accuracy: 0.8165\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.48551\n",
            "Epoch 106/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5020 - accuracy: 0.8139 - val_loss: 0.5451 - val_accuracy: 0.8422\n",
            "\n",
            "Epoch 00106: val_loss did not improve from 0.48551\n",
            "Epoch 107/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5175 - accuracy: 0.8160 - val_loss: 0.5120 - val_accuracy: 0.8557\n",
            "\n",
            "Epoch 00107: val_loss did not improve from 0.48551\n",
            "Epoch 108/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.5230 - accuracy: 0.8113 - val_loss: 0.5101 - val_accuracy: 0.8406\n",
            "\n",
            "Epoch 00108: val_loss did not improve from 0.48551\n",
            "Epoch 109/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4617 - accuracy: 0.8308 - val_loss: 0.6027 - val_accuracy: 0.8095\n",
            "\n",
            "Epoch 00109: val_loss did not improve from 0.48551\n",
            "Epoch 110/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4469 - accuracy: 0.8407 - val_loss: 0.5748 - val_accuracy: 0.8292\n",
            "\n",
            "Epoch 00110: val_loss did not improve from 0.48551\n",
            "Epoch 111/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4334 - accuracy: 0.8462 - val_loss: 0.5547 - val_accuracy: 0.8080\n",
            "\n",
            "Epoch 00111: val_loss did not improve from 0.48551\n",
            "Epoch 112/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4335 - accuracy: 0.8436 - val_loss: 0.6127 - val_accuracy: 0.8101\n",
            "\n",
            "Epoch 00112: val_loss did not improve from 0.48551\n",
            "Epoch 113/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4678 - accuracy: 0.8291 - val_loss: 0.5443 - val_accuracy: 0.8196\n",
            "\n",
            "Epoch 00113: val_loss did not improve from 0.48551\n",
            "Epoch 114/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4164 - accuracy: 0.8517 - val_loss: 0.5946 - val_accuracy: 0.8196\n",
            "\n",
            "Epoch 00114: val_loss did not improve from 0.48551\n",
            "Epoch 115/150\n",
            "46/46 [==============================] - 102s 2s/step - loss: 0.4776 - accuracy: 0.8196 - val_loss: 0.6019 - val_accuracy: 0.8142\n",
            "\n",
            "Epoch 00115: val_loss did not improve from 0.48551\n",
            "Epoch 116/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4907 - accuracy: 0.8167 - val_loss: 0.6537 - val_accuracy: 0.7782\n",
            "\n",
            "Epoch 00116: val_loss did not improve from 0.48551\n",
            "Epoch 117/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 2.0338 - accuracy: 0.7297 - val_loss: 1.8473 - val_accuracy: 0.7932\n",
            "\n",
            "Epoch 00117: val_loss did not improve from 0.48551\n",
            "Epoch 118/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.9256 - accuracy: 0.7897 - val_loss: 0.7600 - val_accuracy: 0.7911\n",
            "\n",
            "Epoch 00118: val_loss did not improve from 0.48551\n",
            "Epoch 119/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4533 - accuracy: 0.8421 - val_loss: 0.6729 - val_accuracy: 0.7930\n",
            "\n",
            "Epoch 00119: val_loss did not improve from 0.48551\n",
            "Epoch 120/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4590 - accuracy: 0.8400 - val_loss: 0.5638 - val_accuracy: 0.8152\n",
            "\n",
            "Epoch 00120: val_loss did not improve from 0.48551\n",
            "Epoch 121/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4310 - accuracy: 0.8495 - val_loss: 0.5005 - val_accuracy: 0.8580\n",
            "\n",
            "Epoch 00121: val_loss did not improve from 0.48551\n",
            "Epoch 122/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4297 - accuracy: 0.8453 - val_loss: 0.5822 - val_accuracy: 0.8124\n",
            "\n",
            "Epoch 00122: val_loss did not improve from 0.48551\n",
            "Epoch 123/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.5066 - accuracy: 0.8139 - val_loss: 0.5152 - val_accuracy: 0.8432\n",
            "\n",
            "Epoch 00123: val_loss did not improve from 0.48551\n",
            "Epoch 124/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4324 - accuracy: 0.8453 - val_loss: 0.6551 - val_accuracy: 0.7590\n",
            "\n",
            "Epoch 00124: val_loss did not improve from 0.48551\n",
            "Epoch 125/150\n",
            "46/46 [==============================] - 105s 2s/step - loss: 0.4457 - accuracy: 0.8410 - val_loss: 0.4762 - val_accuracy: 0.8730\n",
            "\n",
            "Epoch 00125: val_loss improved from 0.48551 to 0.47622, saving model to model_ResNet50-125-0.873-0.476.h5\n",
            "Epoch 126/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.3684 - accuracy: 0.8732 - val_loss: 0.5628 - val_accuracy: 0.8370\n",
            "\n",
            "Epoch 00126: val_loss did not improve from 0.47622\n",
            "Epoch 127/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4177 - accuracy: 0.8488 - val_loss: 0.5104 - val_accuracy: 0.8564\n",
            "\n",
            "Epoch 00127: val_loss did not improve from 0.47622\n",
            "Epoch 128/150\n",
            "46/46 [==============================] - 105s 2s/step - loss: 0.3618 - accuracy: 0.8756 - val_loss: 0.5132 - val_accuracy: 0.8608\n",
            "\n",
            "Epoch 00128: val_loss did not improve from 0.47622\n",
            "Epoch 129/150\n",
            "46/46 [==============================] - 105s 2s/step - loss: 0.4891 - accuracy: 0.8253 - val_loss: 0.5477 - val_accuracy: 0.8256\n",
            "\n",
            "Epoch 00129: val_loss did not improve from 0.47622\n",
            "Epoch 130/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.5009 - accuracy: 0.8146 - val_loss: 0.5412 - val_accuracy: 0.8505\n",
            "\n",
            "Epoch 00130: val_loss did not improve from 0.47622\n",
            "Epoch 131/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.3522 - accuracy: 0.8780 - val_loss: 0.5400 - val_accuracy: 0.8393\n",
            "\n",
            "Epoch 00131: val_loss did not improve from 0.47622\n",
            "Epoch 132/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.3847 - accuracy: 0.8657 - val_loss: 0.6189 - val_accuracy: 0.7961\n",
            "\n",
            "Epoch 00132: val_loss did not improve from 0.47622\n",
            "Epoch 133/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.3777 - accuracy: 0.8638 - val_loss: 0.6997 - val_accuracy: 0.7771\n",
            "\n",
            "Epoch 00133: val_loss did not improve from 0.47622\n",
            "Epoch 134/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4667 - accuracy: 0.8310 - val_loss: 0.5003 - val_accuracy: 0.8580\n",
            "\n",
            "Epoch 00134: val_loss did not improve from 0.47622\n",
            "Epoch 135/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4445 - accuracy: 0.8403 - val_loss: 0.8262 - val_accuracy: 0.7157\n",
            "\n",
            "Epoch 00135: val_loss did not improve from 0.47622\n",
            "Epoch 136/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4025 - accuracy: 0.8602 - val_loss: 0.5701 - val_accuracy: 0.8222\n",
            "\n",
            "Epoch 00136: val_loss did not improve from 0.47622\n",
            "Epoch 137/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4219 - accuracy: 0.8467 - val_loss: 0.6754 - val_accuracy: 0.7676\n",
            "\n",
            "Epoch 00137: val_loss did not improve from 0.47622\n",
            "Epoch 138/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4311 - accuracy: 0.8389 - val_loss: 0.5699 - val_accuracy: 0.8202\n",
            "\n",
            "Epoch 00138: val_loss did not improve from 0.47622\n",
            "Epoch 139/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.3675 - accuracy: 0.8690 - val_loss: 0.5674 - val_accuracy: 0.8222\n",
            "\n",
            "Epoch 00139: val_loss did not improve from 0.47622\n",
            "Epoch 140/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4238 - accuracy: 0.8400 - val_loss: 0.5231 - val_accuracy: 0.8375\n",
            "\n",
            "Epoch 00140: val_loss did not improve from 0.47622\n",
            "Epoch 141/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.3807 - accuracy: 0.8635 - val_loss: 0.5944 - val_accuracy: 0.8186\n",
            "\n",
            "Epoch 00141: val_loss did not improve from 0.47622\n",
            "Epoch 142/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4064 - accuracy: 0.8531 - val_loss: 0.6770 - val_accuracy: 0.7657\n",
            "\n",
            "Epoch 00142: val_loss did not improve from 0.47622\n",
            "Epoch 143/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4348 - accuracy: 0.8379 - val_loss: 0.7401 - val_accuracy: 0.7619\n",
            "\n",
            "Epoch 00143: val_loss did not improve from 0.47622\n",
            "Epoch 144/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.4842 - accuracy: 0.8168 - val_loss: 0.4952 - val_accuracy: 0.8481\n",
            "\n",
            "Epoch 00144: val_loss did not improve from 0.47622\n",
            "Epoch 145/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.3731 - accuracy: 0.8685 - val_loss: 0.5191 - val_accuracy: 0.8577\n",
            "\n",
            "Epoch 00145: val_loss did not improve from 0.47622\n",
            "Epoch 146/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.3458 - accuracy: 0.8770 - val_loss: 0.5164 - val_accuracy: 0.8412\n",
            "\n",
            "Epoch 00146: val_loss did not improve from 0.47622\n",
            "Epoch 147/150\n",
            "46/46 [==============================] - 104s 2s/step - loss: 0.3731 - accuracy: 0.8657 - val_loss: 0.5162 - val_accuracy: 0.8515\n",
            "\n",
            "Epoch 00147: val_loss did not improve from 0.47622\n",
            "Epoch 148/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4549 - accuracy: 0.8384 - val_loss: 0.5013 - val_accuracy: 0.8557\n",
            "\n",
            "Epoch 00148: val_loss did not improve from 0.47622\n",
            "Epoch 149/150\n",
            "46/46 [==============================] - 103s 2s/step - loss: 0.4352 - accuracy: 0.8376 - val_loss: 0.4874 - val_accuracy: 0.8608\n",
            "\n",
            "Epoch 00149: val_loss did not improve from 0.47622\n",
            "Epoch 150/150\n",
            "46/46 [==============================] - 105s 2s/step - loss: 0.3561 - accuracy: 0.8749 - val_loss: 0.5523 - val_accuracy: 0.8386\n",
            "\n",
            "Epoch 00150: val_loss did not improve from 0.47622\n",
            "\n",
            "\n",
            "\n",
            "Time: 16004.957324028015\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfTgMF2bx0nj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        },
        "outputId": "c6fd30ed-ea16-4542-ec8a-4b3c8a3a5fb3"
      },
      "source": [
        "# accuracies\n",
        "plt.plot(r.history['accuracy'], label='train accuracy')\n",
        "plt.plot(r.history['val_accuracy'], label='val accuracy')\n",
        "plt.legend()\n",
        "plt.savefig('accuracy_and_val_accuracy.png', dpi=200)\n",
        "plt.show()\n",
        " \n",
        "# loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.legend()\n",
        "plt.savefig('loss_and_val_loss.png', dpi=200)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuSulGfLx0sd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2766cd2f-4b19-4e63-8d1e-12124ac4d7e1"
      },
      "source": [
        "from glob import glob\n",
        "import os\n",
        " \n",
        "model = load_model('/content/model_ResNet50-125-0.873-0.476.h5')\n",
        "model.summary()\n",
        " \n",
        "data = []\n",
        "labels = []\n",
        "classes = glob('/content/data_64/val1/*') # data/train/* # data/test/*\n",
        "print('Number of classes : ', classes)\n",
        "cur_path = os.getcwd()\n",
        "print('cur_path : ', cur_path)\n",
        "\n",
        "\n",
        "from PIL import Image\n",
        " \n",
        "# Retrieving the images and their labels \n",
        "for i in range(len(classes)):\n",
        "    path = os.path.join(cur_path,'data_64/val1',str(i))\n",
        "    print('path : ',path)\n",
        "    images = os.listdir(path)\n",
        "    # print('images : ',images)\n",
        "    \n",
        "    for a in images:\n",
        "        try:\n",
        "            image = Image.open(path + '/'+ a)            \n",
        "            image = image.resize((224,224))\n",
        "            image = np.array(image)\n",
        "            data.append(image)\n",
        "            labels.append(i)\n",
        "        except:\n",
        "            print(\"Error loading image\")\n",
        "print('loaded ...')\n",
        " \n",
        "\n",
        "\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        " \n",
        "for i in range(len(data)):\n",
        "    x = data[i]\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    img_data = preprocess_input(x)\n",
        "    classes = model.predict(img_data)\n",
        "    li = list(classes[0])\n",
        "    y_pred.append(li.index(max(li)))\n",
        "    y_true.append(labels[i])\n",
        "    \n",
        "    # print('y_true : ', y_true[i], ' y_pred : ', y_pred[i])\n",
        " \n",
        "print('done...')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 224, 224, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv1_pad (ZeroPadding2D)       (None, 230, 230, 3)  0           input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv1_conv (Conv2D)             (None, 112, 112, 64) 9472        conv1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv1_bn (BatchNormalization)   (None, 112, 112, 64) 256         conv1_conv[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv1_relu (Activation)         (None, 112, 112, 64) 0           conv1_bn[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pad (ZeroPadding2D)       (None, 114, 114, 64) 0           conv1_relu[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "pool1_pool (MaxPooling2D)       (None, 56, 56, 64)   0           pool1_pad[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_conv (Conv2D)    (None, 56, 56, 64)   4160        pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_1_relu (Activation (None, 56, 56, 64)   0           conv2_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_2_relu (Activation (None, 56, 56, 64)   0           conv2_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_conv (Conv2D)    (None, 56, 56, 256)  16640       pool1_pool[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_0_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_add (Add)          (None, 56, 56, 256)  0           conv2_block1_0_bn[0][0]          \n",
            "                                                                 conv2_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block1_out (Activation)   (None, 56, 56, 256)  0           conv2_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_1_relu (Activation (None, 56, 56, 64)   0           conv2_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_2_relu (Activation (None, 56, 56, 64)   0           conv2_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_add (Add)          (None, 56, 56, 256)  0           conv2_block1_out[0][0]           \n",
            "                                                                 conv2_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block2_out (Activation)   (None, 56, 56, 256)  0           conv2_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_conv (Conv2D)    (None, 56, 56, 64)   16448       conv2_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_1_relu (Activation (None, 56, 56, 64)   0           conv2_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_conv (Conv2D)    (None, 56, 56, 64)   36928       conv2_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_bn (BatchNormali (None, 56, 56, 64)   256         conv2_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_2_relu (Activation (None, 56, 56, 64)   0           conv2_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_conv (Conv2D)    (None, 56, 56, 256)  16640       conv2_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_3_bn (BatchNormali (None, 56, 56, 256)  1024        conv2_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_add (Add)          (None, 56, 56, 256)  0           conv2_block2_out[0][0]           \n",
            "                                                                 conv2_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2_block3_out (Activation)   (None, 56, 56, 256)  0           conv2_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_conv (Conv2D)    (None, 28, 28, 128)  32896       conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_1_relu (Activation (None, 28, 28, 128)  0           conv3_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_2_relu (Activation (None, 28, 28, 128)  0           conv3_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_conv (Conv2D)    (None, 28, 28, 512)  131584      conv2_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_0_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_add (Add)          (None, 28, 28, 512)  0           conv3_block1_0_bn[0][0]          \n",
            "                                                                 conv3_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block1_out (Activation)   (None, 28, 28, 512)  0           conv3_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_1_relu (Activation (None, 28, 28, 128)  0           conv3_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_2_relu (Activation (None, 28, 28, 128)  0           conv3_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_add (Add)          (None, 28, 28, 512)  0           conv3_block1_out[0][0]           \n",
            "                                                                 conv3_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block2_out (Activation)   (None, 28, 28, 512)  0           conv3_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_1_relu (Activation (None, 28, 28, 128)  0           conv3_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_2_relu (Activation (None, 28, 28, 128)  0           conv3_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_add (Add)          (None, 28, 28, 512)  0           conv3_block2_out[0][0]           \n",
            "                                                                 conv3_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block3_out (Activation)   (None, 28, 28, 512)  0           conv3_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_conv (Conv2D)    (None, 28, 28, 128)  65664       conv3_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_1_relu (Activation (None, 28, 28, 128)  0           conv3_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_conv (Conv2D)    (None, 28, 28, 128)  147584      conv3_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_bn (BatchNormali (None, 28, 28, 128)  512         conv3_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_2_relu (Activation (None, 28, 28, 128)  0           conv3_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_conv (Conv2D)    (None, 28, 28, 512)  66048       conv3_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_3_bn (BatchNormali (None, 28, 28, 512)  2048        conv3_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_add (Add)          (None, 28, 28, 512)  0           conv3_block3_out[0][0]           \n",
            "                                                                 conv3_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv3_block4_out (Activation)   (None, 28, 28, 512)  0           conv3_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_conv (Conv2D)    (None, 14, 14, 256)  131328      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_1_relu (Activation (None, 14, 14, 256)  0           conv4_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_2_relu (Activation (None, 14, 14, 256)  0           conv4_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_conv (Conv2D)    (None, 14, 14, 1024) 525312      conv3_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_0_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_0_bn[0][0]          \n",
            "                                                                 conv4_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block1_out (Activation)   (None, 14, 14, 1024) 0           conv4_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_1_relu (Activation (None, 14, 14, 256)  0           conv4_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_2_relu (Activation (None, 14, 14, 256)  0           conv4_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_add (Add)          (None, 14, 14, 1024) 0           conv4_block1_out[0][0]           \n",
            "                                                                 conv4_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block2_out (Activation)   (None, 14, 14, 1024) 0           conv4_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_1_relu (Activation (None, 14, 14, 256)  0           conv4_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_2_relu (Activation (None, 14, 14, 256)  0           conv4_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_add (Add)          (None, 14, 14, 1024) 0           conv4_block2_out[0][0]           \n",
            "                                                                 conv4_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block3_out (Activation)   (None, 14, 14, 1024) 0           conv4_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_1_relu (Activation (None, 14, 14, 256)  0           conv4_block4_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block4_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block4_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_2_relu (Activation (None, 14, 14, 256)  0           conv4_block4_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_add (Add)          (None, 14, 14, 1024) 0           conv4_block3_out[0][0]           \n",
            "                                                                 conv4_block4_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block4_out (Activation)   (None, 14, 14, 1024) 0           conv4_block4_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block4_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_1_relu (Activation (None, 14, 14, 256)  0           conv4_block5_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block5_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block5_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_2_relu (Activation (None, 14, 14, 256)  0           conv4_block5_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_add (Add)          (None, 14, 14, 1024) 0           conv4_block4_out[0][0]           \n",
            "                                                                 conv4_block5_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block5_out (Activation)   (None, 14, 14, 1024) 0           conv4_block5_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_conv (Conv2D)    (None, 14, 14, 256)  262400      conv4_block5_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_1_relu (Activation (None, 14, 14, 256)  0           conv4_block6_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_conv (Conv2D)    (None, 14, 14, 256)  590080      conv4_block6_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_bn (BatchNormali (None, 14, 14, 256)  1024        conv4_block6_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_2_relu (Activation (None, 14, 14, 256)  0           conv4_block6_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_conv (Conv2D)    (None, 14, 14, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_3_bn (BatchNormali (None, 14, 14, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_add (Add)          (None, 14, 14, 1024) 0           conv4_block5_out[0][0]           \n",
            "                                                                 conv4_block6_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv4_block6_out (Activation)   (None, 14, 14, 1024) 0           conv4_block6_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_conv (Conv2D)    (None, 7, 7, 512)    524800      conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_1_relu (Activation (None, 7, 7, 512)    0           conv5_block1_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block1_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block1_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_2_relu (Activation (None, 7, 7, 512)    0           conv5_block1_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_conv (Conv2D)    (None, 7, 7, 2048)   2099200     conv4_block6_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block1_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_0_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_0_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block1_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_0_bn[0][0]          \n",
            "                                                                 conv5_block1_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block1_out (Activation)   (None, 7, 7, 2048)   0           conv5_block1_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block1_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_1_relu (Activation (None, 7, 7, 512)    0           conv5_block2_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block2_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block2_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_2_relu (Activation (None, 7, 7, 512)    0           conv5_block2_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block2_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block2_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_add (Add)          (None, 7, 7, 2048)   0           conv5_block1_out[0][0]           \n",
            "                                                                 conv5_block2_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block2_out (Activation)   (None, 7, 7, 2048)   0           conv5_block2_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_conv (Conv2D)    (None, 7, 7, 512)    1049088     conv5_block2_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_1_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_1_relu (Activation (None, 7, 7, 512)    0           conv5_block3_1_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_conv (Conv2D)    (None, 7, 7, 512)    2359808     conv5_block3_1_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_bn (BatchNormali (None, 7, 7, 512)    2048        conv5_block3_2_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_2_relu (Activation (None, 7, 7, 512)    0           conv5_block3_2_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_conv (Conv2D)    (None, 7, 7, 2048)   1050624     conv5_block3_2_relu[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_3_bn (BatchNormali (None, 7, 7, 2048)   8192        conv5_block3_3_conv[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_add (Add)          (None, 7, 7, 2048)   0           conv5_block2_out[0][0]           \n",
            "                                                                 conv5_block3_3_bn[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv5_block3_out (Activation)   (None, 7, 7, 2048)   0           conv5_block3_add[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 100352)       0           conv5_block3_out[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 6)            602118      flatten[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 24,189,830\n",
            "Trainable params: 602,118\n",
            "Non-trainable params: 23,587,712\n",
            "__________________________________________________________________________________________________\n",
            "Number of classes :  ['/content/data_64/val1/5', '/content/data_64/val1/3', '/content/data_64/val1/0', '/content/data_64/val1/1', '/content/data_64/val1/2', '/content/data_64/val1/4']\n",
            "cur_path :  /content\n",
            "path :  /content/data_64/val1/0\n",
            "path :  /content/data_64/val1/1\n",
            "path :  /content/data_64/val1/2\n",
            "path :  /content/data_64/val1/3\n",
            "path :  /content/data_64/val1/4\n",
            "path :  /content/data_64/val1/5\n",
            "loaded ...\n",
            "done...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3i2jG7pz7Lr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 834
        },
        "outputId": "758e37bc-70d1-4afb-a401-f89b4b56585b"
      },
      "source": [
        "  # confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(y_true,y_pred)\n",
        "print(matrix, '\\n')\n",
        " \n",
        "# accuracy: (tp + tn) / (p + n)\n",
        "from sklearn.metrics import accuracy_score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "print('Accuracy: %f' % accuracy, '\\n')\n",
        "print('Error rate: %f' % (1-accuracy), '\\n')\n",
        " \n",
        "from sklearn.metrics import classification_report\n",
        "print(\"classification_report \\n\\n\", classification_report(y_true,y_pred), '\\n')\n",
        " \n",
        "# kappa\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "kappa = cohen_kappa_score(y_true,y_pred)\n",
        "print('Cohens kappa: %f' % kappa)\n",
        " \n",
        "# Commented out IPython magic to ensure Python compatibility.\n",
        "import matplotlib.pyplot as plt\n",
        "# %matplotlib inline\n",
        "import seaborn as sns \n",
        " \n",
        "plt.rcParams['font.size'] = 20\n",
        "bg_color = (0,0.77,0.77)\n",
        "plt.rcParams['figure.facecolor'] = bg_color\n",
        "plt.rcParams['axes.facecolor'] = bg_color\n",
        " \n",
        "fig, ax = plt.subplots(1)\n",
        " \n",
        "plt.text(1,-.3, \"Confusion Matrix\", fontsize = 25, color='Black', fontstyle='italic')\n",
        " \n",
        "ax = sns.heatmap(matrix, annot=True, annot_kws={'size':15}, fmt=\"d\", \n",
        "                 robust=True, linewidths=.9, cmap=\"RdYlBu\", square=True)\n",
        "# vmin=0, vmax=350,\n",
        "plt.savefig(\"model_VGG19.jpeg\", dpi=200, facecolor=bg_color, transparent=True)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[  0   0   1  52 160 125]\n",
            " [  0   0   0  75 177 147]\n",
            " [  0   0   0  50 151 240]\n",
            " [  0   0   5  94 358 389]\n",
            " [  0   0   0  43 219 305]\n",
            " [  0   0   1  13 186 103]] \n",
            "\n",
            "Accuracy: 0.143746 \n",
            "\n",
            "Error rate: 0.856254 \n",
            "\n",
            "classification_report \n",
            "\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       338\n",
            "           1       0.00      0.00      0.00       399\n",
            "           2       0.00      0.00      0.00       441\n",
            "           3       0.29      0.11      0.16       846\n",
            "           4       0.18      0.39      0.24       567\n",
            "           5       0.08      0.34      0.13       303\n",
            "\n",
            "    accuracy                           0.14      2894\n",
            "   macro avg       0.09      0.14      0.09      2894\n",
            "weighted avg       0.13      0.14      0.11      2894\n",
            " \n",
            "\n",
            "Cohens kappa: -0.026006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoqrPOEgz7g_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjKy9MPCz7qb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yQwpfVMz7y-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}